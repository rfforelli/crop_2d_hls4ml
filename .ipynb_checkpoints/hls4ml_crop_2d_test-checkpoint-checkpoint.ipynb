{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898338ff",
   "metadata": {},
   "source": [
    "# Crop2d layer for hls4ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e0442",
   "metadata": {},
   "source": [
    "## Compile dummy model w/ 2d crop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77e256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 19:20:06.626881: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/rforelli/miniforge3/lib:\n",
      "2024-06-14 19:20:06.626909: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-06-14 19:20:08.080993: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/rforelli/miniforge3/lib:\n",
      "2024-06-14 19:20:08.081021: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-06-14 19:20:08.081045: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (correlator2.fnal.gov): /proc/driver/nvidia/version does not exist\n",
      "2024-06-14 19:20:08.081247: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 32, 3)]      0         \n",
      "                                                                 \n",
      " cropping2d (Cropping2D)     (None, 100, 20, 3)        0         \n",
      "                                                                 \n",
      " q_conv2d (QConv2D)          (None, 98, 18, 8)         224       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 49, 9, 8)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " q_conv2d_1 (QConv2D)        (None, 47, 7, 16)         1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 23, 3, 16)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1104)              0         \n",
      "                                                                 \n",
      " q_dense (QDense)            (None, 16)                17680     \n",
      "                                                                 \n",
      " q_dense_1 (QDense)          (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,242\n",
      "Trainable params: 19,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, Model\n",
    "from tensorflow.keras.layers import Cropping2D, MaxPooling2D, Flatten\n",
    "from keras.layers import Conv2D, Dense, BatchNormalization, MaxPooling2D, Activation, Flatten, AveragePooling2D, MaxPool2D, Concatenate\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.qconvolutional import QConv2D\n",
    "from qkeras.qpooling import QAveragePooling2D\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu, smooth_sigmoid\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (128, 32, 3)  # 128x32 RGB image\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Cropping layer (cropping to a 100x20 region)\n",
    "# x = Cropping2D(cropping=6)(inputs) # Alternate form, also works with new layer\n",
    "# x = Cropping2D(cropping=(14, 6))(inputs) # Alternate form, also works with new layer\n",
    "x = Cropping2D(cropping=((14, 14), (6, 6)))(inputs)\n",
    "\n",
    "# Smaller convolutional layer\n",
    "x = QConv2D(8, (3, 3), activation='relu', kernel_quantizer=quantized_bits(4, 0, 1), bias_quantizer=quantized_bits(4, 0, 1))(x)\n",
    "\n",
    "# Max pooling layer\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Another smaller convolutional layer\n",
    "x = QConv2D(16, (3, 3), activation='relu', kernel_quantizer=quantized_bits(4, 0, 1), bias_quantizer=quantized_bits(4, 0, 1))(x)\n",
    "\n",
    "# Another max pooling layer\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Flatten the output\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Smaller fully connected layer\n",
    "x = QDense(16, activation='relu', kernel_quantizer=quantized_bits(4, 0, 1), bias_quantizer=quantized_bits(4, 0, 1))(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = QDense(10, activation='softmax', kernel_quantizer=quantized_bits(4, 0, 1), bias_quantizer=quantized_bits(4, 0, 1))(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eccbd143",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10806f36",
   "metadata": {},
   "source": [
    "## Implement custom crop 2d layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70f3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88b8a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hls4ml layer implementation\n",
    "class Crop2D(hls4ml.model.layers.Layer):\n",
    "    '''hls4ml implementation of a hypothetical custom layer'''\n",
    "\n",
    "    def initialize(self):\n",
    "        inp = self.get_input_variable()\n",
    "        shape = [1, self.attributes['out_height'], self.attributes['out_width'], self.attributes['n_chan']]\n",
    "        dims = [f'OUT_HEIGHT_{self.index}', f'OUT_WIDTH_{self.index}', f'N_FILT_{self.index}']\n",
    "        self.add_output_variable(shape, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8849da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser for converter\n",
    "def parse_crop_2d_layer(keras_layer, input_names, input_shapes, data_reader):\n",
    "    layer = {}\n",
    "    layer['class_name'] = 'Crop2D'\n",
    "    layer['name'] = keras_layer['config']['name']\n",
    "    layer['n_in'] = input_shapes[0][1]*input_shapes[0][2]\n",
    "    \n",
    "    cropping = keras_layer['config']['cropping']\n",
    "    crop_top, crop_bottom = cropping[0]\n",
    "    crop_left, crop_right = cropping[1]\n",
    "    \n",
    "    in_height = input_shapes[0][1]\n",
    "    in_width = input_shapes[0][2]\n",
    "    out_height = in_height - crop_top - crop_bottom\n",
    "    out_width = in_width - crop_left - crop_right\n",
    "    n_chan = input_shapes[0][3]\n",
    "    \n",
    "    layer['n_out'] = out_height * out_width\n",
    "    layer['in_height'] = in_height\n",
    "    layer['in_width'] = in_width\n",
    "    layer['out_height'] = out_height\n",
    "    layer['out_width'] = out_width\n",
    "    layer['n_chan'] = n_chan\n",
    "    layer['crop_top'] = crop_top\n",
    "    layer['crop_bottom'] = crop_bottom\n",
    "    layer['crop_left'] = crop_left\n",
    "    layer['crop_right'] = crop_right\n",
    "    outshape = [input_shapes[0][0], out_height, out_width, n_chan]\n",
    "\n",
    "    if input_names is not None:\n",
    "        layer['inputs'] = input_names\n",
    "\n",
    "    return layer, outshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb65b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_2d_config_template = \"\"\"struct config{index} : nnet::crop_2d_config {{\n",
    "    static const unsigned n_in = {n_in};\n",
    "    static const unsigned n_out = {n_out};\n",
    "    static const unsigned in_height = {in_height};\n",
    "    static const unsigned in_width = {in_width};\n",
    "    static const unsigned out_height = {out_height};\n",
    "    static const unsigned out_width = {out_width};\n",
    "    static const unsigned n_chan = {n_chan};\n",
    "    static const unsigned crop_top = {crop_top};\n",
    "    static const unsigned crop_bottom = {crop_bottom};\n",
    "    static const unsigned crop_left = {crop_left};\n",
    "    static const unsigned crop_right = {crop_right};\n",
    "}};\\n\"\"\"\n",
    "\n",
    "crop_2d_function_template = 'nnet::crop_2d<{input_t}, {output_t}, {config}>({input}, {output});'\n",
    "crop_2d_include_list = ['nnet_utils/nnet_crop_2d.h']\n",
    "\n",
    "\n",
    "class Crop2DConfigTemplate(hls4ml.backends.template.LayerConfigTemplate):\n",
    "    def __init__(self):\n",
    "        super().__init__(Crop2D)\n",
    "        self.template = crop_2d_config_template\n",
    "\n",
    "    def format(self, node):\n",
    "        params = self._default_config_params(node)\n",
    "        return self.template.format(**params)\n",
    "\n",
    "\n",
    "class Crop2DFunctionTemplate(hls4ml.backends.template.FunctionCallTemplate):\n",
    "    def __init__(self):\n",
    "        super().__init__(Crop2D, include_header=crop_2d_include_list)\n",
    "        self.template = crop_2d_function_template\n",
    "\n",
    "    def format(self, node):\n",
    "        params = self._default_function_params(node)\n",
    "        return self.template.format(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7268816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the converter for custom Keras layer\n",
    "hls4ml.converters.register_keras_layer_handler('Cropping2D', parse_crop_2d_layer)\n",
    "\n",
    "# Register the hls4ml's IR layer\n",
    "hls4ml.model.layers.register_layer('Crop2D', Crop2D)\n",
    "\n",
    "# Register the optimization passes (if any)\n",
    "backend = hls4ml.backends.get_backend(\"Vivado\")\n",
    "\n",
    "# Register template passes for the given backend\n",
    "backend.register_template(Crop2DConfigTemplate)\n",
    "backend.register_template(Crop2DFunctionTemplate)\n",
    "\n",
    "# Register HLS implementation\n",
    "backend.register_source(os.path.abspath(\"custom_cpp/nnet_crop_2d.h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b4aa6",
   "metadata": {},
   "source": [
    "## Compile hls model & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cdcbb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23122232, 0.07896373, 0.10126586, 0.0944827 , 0.09065075,\n",
       "        0.10397156, 0.04407118, 0.05580137, 0.05235622, 0.14721435]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(1, 128, 32,3)\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97350d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 128, 32, 3]], output shape: [None, 128, 32, 3]\n",
      "Layer name: cropping2d, layer type: Crop2D, input shapes: [[None, 128, 32, 3]], output shape: [None, 100, 20, 3]\n",
      "Layer name: q_conv2d, layer type: QConv2D, input shapes: [[None, 100, 20, 3]], output shape: [None, 98, 18, 8]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, input shapes: [[None, 98, 18, 8]], output shape: [None, 49, 9, 8]\n",
      "Layer name: q_conv2d_1, layer type: QConv2D, input shapes: [[None, 49, 9, 8]], output shape: [None, 47, 7, 16]\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D, input shapes: [[None, 47, 7, 16]], output shape: [None, 23, 3, 16]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 23, 3, 16]], output shape: [None, 1104]\n",
      "Layer name: q_dense, layer type: QDense, input shapes: [[None, 1104]], output shape: [None, 16]\n",
      "Layer name: q_dense_1, layer type: QDense, input shapes: [[None, 16]], output shape: [None, 10]\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 128, 32, 3]], output shape: [None, 128, 32, 3]\n",
      "Layer name: cropping2d, layer type: Crop2D, input shapes: [[None, 128, 32, 3]], output shape: [None, 100, 20, 3]\n",
      "Layer name: q_conv2d, layer type: QConv2D, input shapes: [[None, 100, 20, 3]], output shape: [None, 98, 18, 8]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, input shapes: [[None, 98, 18, 8]], output shape: [None, 49, 9, 8]\n",
      "Layer name: q_conv2d_1, layer type: QConv2D, input shapes: [[None, 49, 9, 8]], output shape: [None, 47, 7, 16]\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D, input shapes: [[None, 47, 7, 16]], output shape: [None, 23, 3, 16]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 23, 3, 16]], output shape: [None, 1104]\n",
      "Layer name: q_dense, layer type: QDense, input shapes: [[None, 1104]], output shape: [None, 16]\n",
      "Layer name: q_dense_1, layer type: QDense, input shapes: [[None, 16]], output shape: [None, 10]\n",
      "Creating HLS model\n",
      "WARNING: Layer q_conv2d requires \"dataflow\" pipeline style. Switching to \"dataflow\" pipeline style.\n",
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "config = hls4ml.utils.config_from_keras_model (model, default_precision = 'ap_fixed<16,8>', granularity = 'name')\n",
    "\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(model, output_dir=f\"hls4ml_crop_2d_test\", \n",
    "                                                       backend=\"Vivado\", part='xcku035-fbva676-2-e',\n",
    "                                                       io_type='io_stream', hls_config=config)\n",
    "\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22eced05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22265625, 0.078125  , 0.1015625 , 0.1015625 , 0.1015625 ,\n",
       "       0.1015625 , 0.046875  , 0.0625    , 0.0625    , 0.1328125 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hls_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b8928ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hls_model.build(csim=True, synth=True, cosim=False, validation=False, vsynth=False, fifo_opt=False, export=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion_fastml",
   "language": "python",
   "name": "fusion_fastml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
